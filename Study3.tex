
\chapter{Effects of Anthropometric Fidelity on Reaching Boundary Estimation}

%Enhancing Proprioception via Self-Avatars Calibration on Spatial Perception in IVE

The interaction with the IVE could be through an immersive self-avatar which is life-size digital representation of the user. 

Avatars are digital representations of human users in virtual environments. In most VR applications, avatars are the digital representation of the users from either a third person or a first person perspective. A life size visual representation of the user from a first person perspective is knows as immersive self-avatar where the user's body is co-located with its virtual representation. Research has shown the presence of an avatar in the virtual environment affects how people perceive their environment. It also influences the user's behavior on the presence of the other virtual agents in IVE \cite{HUH10,SAD+06,ZUG+07}. Recent perception research suggests the presence of an avatar influences the user's spatial perception in medium field in IVEs \cite{MCW+10,LNW+03,WJS+08}. However, investigating the effect of self-avatar on user's spatial perception in near-field requires accurate body and hand tracking along with an realistic animation depicting user's motion. Many techniques have been developed to improve the real-time rendering and animation of an immersive self-avatar in IVEs. 



%The fidelity of the self-avatar has so 
%induce the body ownership for it to be effective, recent research showed in the presence of specific types of synchronous multi-sensory and sensorimotor simulation \cite{SPD+09,SPD+08} 



Research in inexpensive tracking technologies, such as the Microsoft Kinect, and rich animation techniques are required to produce high quality self-representations in IVE. While techniques are being developed for new and improved methods of real-time rendering and animation of immersive self-avatars in VR with high fidelity, there are some early results that suggest that just the perception of a self-representation via an avatar improves a sense of presence [86, 88]. Also research suggests that when presented with cue conflicts between visual and proprioceptive position of a person’s hand in the VR, we have a strong tendency to resolve the conflict in favor of the visual position (even when the hand was not an articulated or animated realistic hand) [89, 90]. Recent perception research suggests that simply viewing a rendering of one’s static feet in the VR decreases the depth compression of participants in a bisection task within a simulation with a HMD [91]. Researchers have found that the presence of a static self-avatar and the experience of walking around with a self-avatar have the effect of improving distance estimation when later measured via blind walking techniques [92, 93]. The limitations of the small number of related work investigating the impact of self-avatars on spatial perception are as follows: a) Self-avatars were generally experienced in an exploration phase which usually occurred in a different environment from the distance judgment phase of the experiment; therefore self-avatars were not involved in calibration of a complex manual fine motor tasks requiring depth estimation. b) The perception of the self-avatar has only been investigated in these recent studies in far distances (action space) [92, 93], which were measured using blind walking techniques. Typically when viewers see targets at these distances, the limited field-of-view of the HMD occludes the perception of the immersive self-avatars during the depth estimation task. c) Self-avatars were not involved in the viewers’ responses or actions to the visual stimuli; therefore, the calibration effect of the immersive self-avatar in scaling users’ responses in VR to depth perception is currently unknown.

Another body of research has looked at the visual fidelity of avatars in IVE. Volante et al. \cite{VBC+16} investigated the effect of the visual fidelity of the avatar on users' behavioral and emotional responses. They showed that users in visually realistic avatar condition expressed more of the expected emotion towards the avatar as compare to non-photo-realistic conditions. In another study, Lok et al. \cite{LNW+03} compared the real world avatar hand with the virtual and a hybrid representation of it and found no significant effect between the conditions. Regarding virtual humans in IVE, McDonnel et al. \cite{MBB12} found that more realistic avatars can be even more disturbing to the users as compared to less realistic avatars due to the uncanny valley effect. However, there has been no evidence of this phenomenon regarding self-avatars. In another study, Lin and J{\"o}rg \cite{LJ16} showed participants to a degree responded to threats in all the conditions from realistic hand to non-anthropomorphic block model. However, the users' responses were strongest in the realistic hand condition and weakest in the wooden block model condition. They concluded that synchronize movements of the avatar hand with the real one was one of the main factors on inducing the sense of presence and the ownership of the virtual hand. Similarly, Ma and Hommel \cite{MH15} evaluated the hand ownership illusion in situations involving an active operation of the end effector. They found an enhancement on the impression of the hand ownership when coupled with the real hand movements. Overall, the visual fidelity or the rendering style of the avatar and also active operation of the self-avatar have been extensively studied over the last few year in IVE. However, it is less known about the effect of the visual fidelity of the self-avatar on user's spatial perception in near field in IVE.

\section{Hypothesis}

There is little or no research on the visuo-motor calibration effects of immersive self-avatars on spatial judgments in interaction space in IVE. This study has four primary hypotheses. First, we predict that just the existing of the self-avatar will calibrate user's interaction space depth perception in an IVE. Therefore, participants spatial judgments will be improved after the calibration phase regardless of the visual fidelity. Second, the immersive self-avatar visual fidelity has an impact on user's spatial judgments in an IVE. We expect to observe different levels of improvement based on the self-avatar details presented to the participants. Third, we predict that participants in high-fidelity self-avatar condition have a significantly better spatial judgment as compared to medium and low-fidelity avatar conditions. Forth, we expect the adaptation happens fastest for high-fidelity avatar and slowest for low-fidelity avatar in calibration phase and reversion back to normal occurs fastest for low-fidelity avatar and slowest for high-fidelity avatar in posttest phase.


\section{Experiment Methodology}

\subsection{Participants}
Thirty-six undergraduate students will be recruited from the student population of Clemson University and received course credit for their participation. Participants will be required to be right handed as all equipment to be used is for right-handed participants. As participants enter the testing area, they will be given a brief overview of the purpose of the experiment and informed consent will be obtained. All participants will be tested for visual stereo acuity. Participants will be randomly assigned to one the three conditions described in Section \ref{ExpDesign}. 

\subsection{General Setup}
Figure 1 ??? (new figure) depicts the proposed apparatus to be used in the experiment which will be represented in VR. Participants will be seated in a wooden chair, which will be situated approximately 20 cm from the edge of the wooden table. The tabletop will be 50 cm wide by 130 cm long, and will be 76.2 cm tall (which is standard table height). Seat height will vary between 43 and 48 cm depending on the height of the participant. Shorter participants will be allowed to sit on a cushion if they desire. The center of the table will be aligned with the midpoint between the participants’ right eye and right shoulder. Participants will be outfitted with six Pohlemus sensors: 1. On their forehead. 2. On their neck. 3. On their shoulder. 4. On their elbow. 5. On their wrist. And 6. On their hand. Aside from the sensor on the forehead, the other five sensors will all be placed on the bony protrusions at those points on the body. The base for the Pohlemus system was located underneath the table and out of view of the participants. The virtual environment, which will be a recreation of the room the experiment will be performed in, will be displayed using a HTC VIVE HMD.

As mentioned previously, three different avatars regarding different visual fidelities will be utilized in the experiment. In any given block of trials participants will be asked to reach for a target with their right arm and hand. In the real world, participants will be given a Vive controller to hold. The Vive controller is 26.5 cm long from base to tip, 3 cm wide at the base of the handle, 5 cm wide at the top of the handle, 3 cm deep at the handle, and is 12 cm wide at its widest point. The Vive controller will allow the experimenters to accurately model participants wrist position in VR. The controller will be outfitted with a plastic mold that can hold a 10 cm wooden rod with a rubber tip. (add the brace description here)

Participants will be asked to reach for a visual target stimuli in IVE. For any trial, the target will consist of a virtual representation of three illuminated LED lights. The middle light in the target will correspond to the target distance, and with the other two lights illuminated the length of the target area will be three cm. Targets will be presented at 13 different distances, ranging from 20.5 cm to 121.5cm. The difference between each target will be approximately eight cm. Each target will be presented five times each for a total of 65 reaches per phase. 

\subsection{Visual aspects}
An HTC Vive HMD weighing about 563 g will be used for the experiment. The HMD contains Fresnel lens with a resolution of 1200 x 1080 pixels for viewing a stereoscopic virtual environment with IPD adjustment. The field of view of the HMD was determined to be 110 degrees horizontal and 113 degrees vertical. The simulation will be consisted of the virtual model of the experimental room and apparatus created using Blender and Unity3D. The virtual replica of the apparatus include Table, chair, HTC Vive controllers with a plastic mold, and wooden rod. (Add a picture) %A virtual body seated on the chair was also presented to provide an egocentric representation of self whether the participant looked down [Figure \ref{fig:virtualBody}].

\section{Procedure}
As participants enter the testing area, they will be given a brief overview of the purpose of the experiment and informed consent will be obtained. All participants will be asked to sit on the wooden chair at one end of the wooden table. Various motion sensors will be placed on the participant through the use of a long sleeve shirt. 

Before any trials occur, various anthropometric measurements of the participant will be collected using the HTC Vive controller. The experimenter will measure participant standing height (floor to top of head) and various aspects of their arm, such as the length from acromion process to lateral epicondyle of the humerus (shoulder to elbow), and the length from the lateral epicondyle of the humerus to the end of the index finger (elbow to end of index finger). The experimenter will also measure various aspects of the participants arm relative to the positions of the sensors. %The experimenter will also collect participant shoulder height from the floor when the participant is seated, measured as floor to acromion process. 

After these measurements have been collected, the participant will participate in three body ownership tasks in VR. The tasks are based upon those frequently used by Slater in his research on presence in VR. In the different conditions, participants will observe the self-avatar holding a tool. To induce a feeling of embodiment, participants will perform two tasks prior to the experiment. First, from a first-person viewpoint, participants will be able to see their movements in the mirror where the self-avatar movements are synchronized with their actual body movements. After five minutes of performing a set of predefined movements in front of mirror participants will progress to the second task. In the next task, participants will use the HTC Vive controllers to tap different parts of their body such as their shoulders, chest, hip, etc. with a synchronous visuo-tactile stimulus. In total, the body ownership tasks should last for about ten minutes. 

Next, the experimenter will demonstrate the types of reaches that are appropriate in the experiment. Participants will be instructed to reach as quickly and as accurately as possible on each trial. The major restriction participants will have is that they must remain seated (meaning stay on the seat pan) during any attempted reach. During the course of the actual reach participants may engage their arm only, or may engage their entire upper body (i.e. bending at the waist to reach further).

Regardless of phase, each trial will begin with the participant resting their right arm on the armrest of the chair and their back against the back of the chair. Participants will be instructed that this is the starting point for each trial. To ensure uniformity in starting positions across participants, it will be emphasized to participants that their starting posture is critical for the study.

\subsection{Pretest}
In the pretest, participants will be instructed to reach to the target that will appear on the table at various distances from them.  As part of each trial the participant will be asked to perform a reach if they believe they can reach the target. After viewing the target and at the initiation of their reach, participants will be shown a grey screen to simulate closing their eyes but maintaining the same overall illumination, and reach out with the stylus and place the tip of the stylus as close to the center of the target as possible.  After attempting to reach the target, participants will be instructed to return their hand and arm to the starting point to begin the next trial.  If they do not believe they can reach the target they will be instructed to say “no”.  Regardless of condition, all participants will perform the pretest with a high-fidelity avatar. In this phase, participants will only receive haptic feedback from when the controller they are wielding in the real world contacts the surface of the table.

\subsection{Calibration phase}
After the pretest, participants will complete the calibration phase. The task in this phases will be the exact same as in the pretest, except they will perform less reaches to fewer distances. Participants will only be presented with nine different distances, and each distance will be presented five times.  The first six distances presented in the calibration phase will all be reachable targets to encourage participants to engage in a reach.  After the sixth trial, distances that are unreachable will be presented as well. None of the nine distances in the calibration phase will be identical to the 13 distances presented in the pre and posttest. 

The primary manipulation of the experiment will occur in the calibration phase.  Participants will experience one of the following conditions: a) high-fidelity avatar b) medium-fidelity avatar or c) low-fidelity avatar. Participants will not be informed about the changes in the visual fidelity. In this phase, participants will receive haptic feedback from when the controller they are wielding in the real world contacts the surface of the table. Then, once contact has been made participants will be allowed to open their eyes and adjust their reach so the end of the virtually presented hand is in the center of the target, thus receiving visual feedback as well.

\subsection{Posttest}
The posttest will be identical to the pretest.  All participants will complete the exact same reaching task while using a high-fidelity avatar arm.  In this phase, participants will only receive haptic feedback from when the controller they are wielding in the real world contacts the surface of the table.
Importantly, the experimenters will ensure there is no delay between the calibration phase and the posttest. By doing so, we hope to preserve the just modified action capabilities of the avatar for the posttest, as a long delay between these two phases might cause the calibration to disappear.

\subsection{Post Data Collection}
After the conclusion of data collection, the experimenter will again measure various aspects of the participants arm to ensure that the positions of the sensors did not move over the course of the experiment. In addition, the participant will be asked to perform two maximum reaches with their arm only (reaching their arm straight out as far as they can without engaging their shoulder or back) and two maximum reaches with their entire upper body (by reaching as far as they can and touching the table with no restrictions other than remaining seated in the chair). Lastly, participants will be given a brief questionnaire designed to measure the degree of body ownership they felt over the avatar in VR. A manipulation check will also be administered to participants. They will be asked if they noticed anything odd that occurred during the course of the experiment.

\section{Experiment Design} \label{ExpDesign}
The proposed experiment will utilize a 3 (Condition: High-fidelity, Medium-Fidelity, Low-fidelity Avatar) by 3 (Phase: PreTest, Calibration, PostTest) mixed groups design. Condition will be a between subjects variable and phase will be a within subjects variable. All three conditions will involve use of an avatar’s arm that is directly proportional to the dimensions of the user’s own arm where the visual fidelity will be altered in the calibration phase. 

\section{Data Preprocessing}

\section{Expected Results}
We will use HTC Vive and Polhemus electromagnetic tracking system to track and create accurately scaled articulated models of the users and animate the limbs to match the physical reaching activities in our perception apparatus described in previous section using the \textit{Inverse Kinematic} (IK). We have plan to use a Synertial tracking suit as a confirmation to our IK system to more accurately animate the user's upper body movements. The anthropometric measurement prior to the experiment will ensure that the participant’s limbs are scaled accordingly. Together with a tracked stylus, participants can receive accurate visual feedback of their actions (proprioception) in a closed-loop condition via the immersive self-avatar in the IVE. In order to assess the impact of visuo-motor calibration to enhanced proprioception via the immersive self-avatar, participants will perform the closed-loop calibration phase in one of three visual fidelity conditions: 1) high-fidelity: realistic limb matching the size and scale of the participant’s limbs, 2) medium-fidelity: abstract rendering of joint and limb location only, and 3) low-fidelity: abstract rendering of joint locations only. We expect to find that the high-fidelity self-avatar of the participant’s arm will provide a more realistic proprioceptive cue as compared to medium and low-fidelity avatar, and will significantly enhance depth perception in the IVE. 

